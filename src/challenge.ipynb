{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h3w562-HDuN"
      },
      "outputs": [],
      "source": [
        "#! pip install memory-profiler\n",
        "#! pip install emoji\n",
        "#! pip install py-spy\n",
        "#! pip install pyspark\n",
        "#! pip install apache-beam[gcp]\n",
        "! pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZNs9KN5sPs-"
      },
      "source": [
        "# Data Engineer Challenge\n",
        "## Recursos\n",
        "* Debido a que solo cuento con el computador de la empresa, no puedo instalar elementos adicionales sin levantar un ticket. Por esta razón, utilicé Google Colab para resolver el desafío.\n",
        "* Actualmente no tengo acceso a una nube con créditos para resolver el desafío y verificar los resultados. Sin embargo, mencionaré las herramientas de la nube que habría utilizado en cada caso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99HzK-O16Nto"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "file_path = 'farmers-protest-tweets-2021-2-4.json'\n",
        "os.environ['FILE_PATH'] = file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZu_y8GDvBAY"
      },
      "source": [
        "## Q1\n",
        "### Aclaraciones\n",
        "* Este caso lo abordé como un problema de BI donde no se requiere un tiempo de respuesta en tiempo real. Por lo tanto, utilicé Spark para optimizar tanto el uso de memoria como el tiempo de procesamiento.\n",
        "\n",
        "* En este ejercicio donde tuve recursos limitados se comprueba que las soluciones optimizando tiempo se aprovecharían mas en ambientes distribuidos, y que la optimización de memoria funciona mejor en ambientes con recursos limitados, tal como DataProc\n",
        "\n",
        "* No modifique el formato de la funcion que se me indicó, pero me hubiera gustado agregar un parametro para pasar la session de spark como parametro y que esta se hiciera fuera de la funcion.\n",
        "\n",
        "* En un servicio en la nube hubiera cargado los datos a bucket de cloud storage, luego ingestado a un tabla de BQ, aplicar filtros de lipienza y calidad del dato segun corresponda y finalmente relizado una consulta que me diera el resultado, esta se podria pasar a DBT para realizar el respectivo versionado y ejecucion periodica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knzNzmYO22Kd"
      },
      "outputs": [],
      "source": [
        "import q1_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIEuj_CJxIZC"
      },
      "source": [
        "#### q1_time\n",
        "\n",
        "* Se puso en cache el df para evitar ser recalculado, aunque la escritura en memoria afecta gravente el tiempo de ejecucion\n",
        "* Se hace un una reduccion por llaves para minimizar shuffles e intentar con esto reducir el tiempo, sin embargo el proceso es costo y requiere mas tiempo del que se ahorra al momento de ejecutar el filtro isin\n",
        "* se hizo broadcast a la columna fecha del df top_dates para minizar shuffles\n",
        "\n",
        "*   La logica utilizada fue la misma para los dos casos:\n",
        " 1. Lectura del archivo como Json\n",
        " 2. Transformar la columna date a tipo timestamp y luego a tipo date\n",
        " 3. Contar tweets por fecha y orfanizar de forma descendente\n",
        " 4. Tomar los 10 primeros\n",
        " 5. Filtrar solo los dias con mas tweets\n",
        " 6. Contar agrupando por dia y nombre de usuario\n",
        " 7. Agrega una columna con la funcion de ventana particionada por fecha y ordena por el conteo previo\n",
        " 8. seleccionar columnas necesarias\n",
        " 9. Capturar y retornar cada linea como una tupla\n",
        "\n",
        " ##### Explicación codigo\n",
        "\n",
        "``` py\n",
        "    # Iniciar la sesion de Spark\n",
        "    spark = SparkSession.builder.appName(\"TweetAnalysis\").getOrCreate()\n",
        "\n",
        "    # Leer el archivo como Json\n",
        "    dfRaw = spark.read.json(file_path)\n",
        "\n",
        "    # Lectura y preprocesamiento del dataframe\n",
        "    df = dfRaw.select(\n",
        "\n",
        "      # Seleccionar la columna del nombre de usuario dentro de usuario\n",
        "      col(\"user.username\").alias(\"username\"),\n",
        "\n",
        "      # Transforma la columna 'date', a timestamp y luego a date\n",
        "      to_date(to_timestamp(col(\"date\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\")).alias(\"date\")\n",
        "      )\n",
        "    \n",
        "    # Cachear df para evitar recalcularlo\n",
        "    df.cache()\n",
        "    \n",
        "    # Top 10 días con más tweets (usando reduceByKey para minimizar shuffles)\n",
        "    top_dates = (\n",
        "\n",
        "      # Convertir a RDD\n",
        "      df.rdd\n",
        "\n",
        "        # Mapear cada fila a una tupla\n",
        "        .map(lambda row: (row.date, 1))\n",
        "\n",
        "        # Agrupar por fecha y sumar los conteos de tweets para cada fecha\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "        # Ordenar las fechas por conteo de tweets en orden descendente\n",
        "        .sortBy(lambda item: item[1], ascending=False)\n",
        "\n",
        "        #Tomar los primeros 10\n",
        "        .take(10)\n",
        "        )\n",
        "    \n",
        "    # Broadcast de top_dates para evitar shuffles en el filtro\n",
        "    top_dates_broadcast = spark.sparkContext.broadcast([date for date, _ in top_dates])\n",
        "\n",
        "    # Filtrar por top días y calcular el usuario con más tweets\n",
        "    day_user = (\n",
        "\n",
        "      #Filtra las fechas con respecto a las fechas que se tomaron antes\n",
        "      df.filter(col(\"date\").isin(top_dates_broadcast.value))\n",
        "\n",
        "        #Agrupa por fecha y usuario\n",
        "        .groupBy(\"date\", \"username\")\n",
        "\n",
        "        #Hace conteo\n",
        "        .count()\n",
        "\n",
        "        #Agrega una columna, con la funcion de ventana particionada por fecha y ordenada por el conteo previo\n",
        "        .withColumn(\"rn\", row_number().over(Window.partitionBy(\"date\").orderBy(desc(\"count\"))))\n",
        "\n",
        "        #Filtra el registro con mas tweets\n",
        "        .filter(col(\"rn\") == 1)\n",
        "\n",
        "        #Elimina la columna antes creada\n",
        "        .drop(\"rn\")\n",
        "\n",
        "        #Selecciona solo fecha y usuario\n",
        "        .select(\"date\", \"username\")\n",
        "        )\n",
        "\n",
        "    # Unpersist de df para liberar la memoria\n",
        "    df.unpersist()\n",
        "    \n",
        "    # Se capura el resultado y se transforma en una lista de tuplas.\n",
        "    [tuple(row) for row in day_user.collect()]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISlGZHeQfdA1",
        "outputId": "5663380e-7457-4b33-bd3e-957f37c98ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: /content/q1_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     5    309.2 MiB    309.2 MiB           1   @memory_profiler.profile\n",
            "     6                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "     7                                           \"\"\"\n",
            "     8                                           Obtiene los usuarios con más tweets por día para los 10 días con mayor actividad.\n",
            "     9                                           \n",
            "    10                                           Esta función lee un archivo JSON de tweets, identifica los 10 días con \n",
            "    11                                           mayor cantidad de tweets y determina el usuario con más tweets para cada \n",
            "    12                                           uno de esos días. La función está optimizada para el tiempo de procesamiento \n",
            "    13                                           utilizando Spark para el análisis distribuido de datos.\n",
            "    14                                           \n",
            "    15                                           Args:\n",
            "    16                                             file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
            "    17                                           \n",
            "    18                                           Returns:\n",
            "    19                                             List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla \n",
            "    20                                             contiene la fecha (datetime.date) y el nombre de usuario (str) \n",
            "    21                                             del usuario con más tweets para ese día.\n",
            "    22                                           \n",
            "    23                                           Raises:\n",
            "    24                                             FileNotFoundError: Si el archivo especificado en `file_path` no se encuentra.\n",
            "    25                                             Exception: Si ocurre cualquier otro error durante el procesamiento.\n",
            "    26                                           \"\"\"\n",
            "    27    309.2 MiB      0.0 MiB           1     from pyspark.sql import SparkSession\n",
            "    28    309.2 MiB      0.0 MiB           1     from pyspark.sql.window import Window\n",
            "    29    309.2 MiB      0.0 MiB           1     from pyspark.sql.functions import col, to_date, to_timestamp, row_number, desc, col\n",
            "    30    309.2 MiB      0.0 MiB           1     try:\n",
            "    31    309.2 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"TweetAnalysis\").getOrCreate()\n",
            "    32    309.2 MiB      0.0 MiB           1       dfRaw = spark.read.json(file_path)\n",
            "    33                                         \n",
            "    34                                             # Lectura y preprocesamiento del dataframe\n",
            "    35    309.2 MiB      0.0 MiB           2       df = dfRaw.select(\n",
            "    36    309.2 MiB      0.0 MiB           1         col(\"user.username\").alias(\"username\"),\n",
            "    37    309.2 MiB      0.0 MiB           1         to_date(to_timestamp(col(\"date\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\")).alias(\"date\")\n",
            "    38                                               )\n",
            "    39                                             \n",
            "    40                                             # Cachear df para evitar recalcularlo\n",
            "    41    309.2 MiB      0.0 MiB           1       df.cache()\n",
            "    42                                             \n",
            "    43                                             # Top 10 días con más tweets (usando reduceByKey para minimizar shuffles)\n",
            "    44    309.2 MiB      0.0 MiB           1       top_dates = (\n",
            "    45    309.2 MiB      0.0 MiB           1         df.rdd\n",
            "    46    309.2 MiB      0.0 MiB           1           .map(lambda row: (row.date, 1))\n",
            "    47    309.2 MiB      0.0 MiB           1           .reduceByKey(lambda a, b: a + b)\n",
            "    48    309.2 MiB      0.0 MiB           1           .sortBy(lambda item: item[1], ascending=False)\n",
            "    49    309.2 MiB      0.0 MiB           1           .take(10)\n",
            "    50                                                 )\n",
            "    51                                             \n",
            "    52                                             # Broadcast de top_dates para evitar shuffles en el filtro\n",
            "    53    309.2 MiB      0.0 MiB          13       top_dates_broadcast = spark.sparkContext.broadcast([date for date, _ in top_dates])\n",
            "    54                                         \n",
            "    55                                             # Filtrar por top días y calcular el usuario con más tweets\n",
            "    56    309.2 MiB      0.0 MiB           1       day_user = (\n",
            "    57    309.2 MiB      0.0 MiB           1         df.filter(col(\"date\").isin(top_dates_broadcast.value))\n",
            "    58    309.2 MiB      0.0 MiB           1           .groupBy(\"date\", \"username\")\n",
            "    59    309.2 MiB      0.0 MiB           1           .count()\n",
            "    60    309.2 MiB      0.0 MiB           1           .withColumn(\"rn\", row_number().over(Window.partitionBy(\"date\").orderBy(desc(\"count\"))))\n",
            "    61    309.2 MiB      0.0 MiB           1           .filter(col(\"rn\") == 1)\n",
            "    62    309.2 MiB      0.0 MiB           1           .drop(\"rn\")\n",
            "    63    309.2 MiB      0.0 MiB           1           .select(\"date\", \"username\")\n",
            "    64                                                 )\n",
            "    65                                         \n",
            "    66                                             # Unpersist de df\n",
            "    67    309.2 MiB      0.0 MiB           1       df.unpersist()\n",
            "    68                                             \n",
            "    69                                             # Se devuelve el resultado como una lista de tuplas.\n",
            "    70    309.2 MiB      0.0 MiB          13       return [tuple(row) for row in day_user.collect()]\n",
            "    71                                         \n",
            "    72                                           except FileNotFoundError:\n",
            "    73                                               print(f\"Error: El archivo '{file_path}' no se encuentra.\")\n",
            "    74                                               raise\n",
            "    75                                           except Exception as e:\n",
            "    76                                               print(f\"Error durante el procesamiento: {e}\")\n",
            "    77                                               raise\n",
            "    78                                           finally: \n",
            "    79    309.2 MiB      0.0 MiB           1       spark.stop()\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
              " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
              " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
              " (datetime.date(2021, 2, 15), 'jot__b'),\n",
              " (datetime.date(2021, 2, 16), 'jot__b'),\n",
              " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
              " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
              " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
              " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
              " (datetime.date(2021, 2, 23), 'Surrypuria')]"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q1_time.q1_time(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGmoEYW5391-",
        "outputId": "a0425b17-3fcd-4462-b9a2-e7f2a2e3b535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Sampling process 100 times a second. Press Control-C to exit.\n",
            "\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/02/13 06:42:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.03s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.34s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.72s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.33s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.01s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.16s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.32s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.05s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.74s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.35s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.79s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.05s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.78s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.36s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.10s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.80s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.66s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.87s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.74s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.34s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.74s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.28s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.05s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.63s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.85s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.30s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.28s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.51s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.15s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.70s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.02s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.72s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.68s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.22s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.87s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.11s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.26s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.29s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.16s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.68s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.80s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.10s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.62s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.06s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.01s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.46s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.24s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.65s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.42s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.75s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.38s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.73s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.33s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Stopped sampling because process exited\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Wrote flamegraph data to 'q1_time.svg'. Samples: 2704 Errors: 0\n"
          ]
        }
      ],
      "source": [
        "!py-spy record -o q1_time.svg -- python -c \"import os; import q1_time; q1_time.q1_time(os.environ.get('FILE_PATH'))\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyZgbsz526wA"
      },
      "outputs": [],
      "source": [
        "import q1_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zhXJsB69YHN"
      },
      "source": [
        "### q1_memory\n",
        "* Se intenta minimizar las variables utilizadas\n",
        "* Se intenta ser lo mas concreto en las acciones para evitar reprocesos\n",
        "*   La logica utilizada fue la misma para los dos casos:\n",
        " 1. Lectura del archivo como Json\n",
        " 2. Transformar la columna date a tipo timestamp y luego a tipo date\n",
        " 3. Contar tweets por fecha y orfanizar de forma descendente\n",
        " 4. Tomar los 10 primeros\n",
        " 5. Filtrar solo los dias con mas tweets\n",
        " 6. Contar agrupando por dia y nombre de usuario\n",
        " 7. Agrega una columna con la funcion de ventana particionada por fecha y ordena por el conteo previo\n",
        " 8. seleccionar columnas necesarias\n",
        " 9. Capturar y retornar cada linea como una tupla\n",
        " #### Explicación código\n",
        "\n",
        "\n",
        "\n",
        "``` py\n",
        "    # Iniciar sesion de spark\n",
        "    spark = SparkSession.builder.appName(\"TweetAnalysis\").getOrCreate()\n",
        "\n",
        "    #Leer archivo json\n",
        "    dfRaw = spark.read.json(file_path)\n",
        "\n",
        "    # Se extrae la fecha y el usuario, convirtiendo la fecha al formato correcto.\n",
        "    df = dfRaw.select(\n",
        "\n",
        "        # Seleccionar la columna del nombre de usuario dentro de usuario  \n",
        "        col(\"user.username\").alias(\"username\"),\n",
        "\n",
        "        # Transformar la columna 'date', a timestamp y luego a date\n",
        "        to_date(to_timestamp(col(\"date\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\")).alias(\"date\"),\n",
        "      )\n",
        "\n",
        "    # Se obtienen los top N días con más tweets.\n",
        "    top_dates = (\n",
        "\n",
        "        # agrupar por fecha\n",
        "        df.groupBy(\"date\")\n",
        "        \n",
        "        # Contar agrupando por fecha\n",
        "        .count()\n",
        "\n",
        "        #Ordenar por el conteo descendente\n",
        "        .orderBy(desc(\"count\"))\n",
        "\n",
        "        #Tomar solo la fecha\n",
        "        .select(\"date\")\n",
        "\n",
        "        #tomar los 10 primeros resultados\n",
        "        .limit(10)\n",
        "\n",
        "        # Capturarlos y colectarlos\n",
        "        .rdd.flatMap(lambda x: x)\n",
        "        .collect()\n",
        "      )\n",
        "\n",
        "    # Se define la ventana para obtener el usuario con más tweets por día.\n",
        "    window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
        "\n",
        "    # Se filtra por los top N días, se agrupa por fecha y usuario, se cuenta\n",
        "    # y se selecciona el usuario con más tweets (rn = 1).\n",
        "    day_user = (\n",
        "\n",
        "      # Filtrar solo las fechas con mas tweets, calculado previamente\n",
        "      df.filter(col(\"date\").isin(top_dates))\n",
        "\n",
        "      #Agrupar por fecha y usuario\n",
        "      .groupBy(\"date\", \"username\")\n",
        "\n",
        "      # Hacer conteo agrupando por fecha y usuario\n",
        "      .count()\n",
        "\n",
        "      # Aplicar la funcion de ventana para organizar de mayor a menor los usuarios que mas con mas tweets ese dia\n",
        "      .select(col(\"date\"), col(\"username\"), row_number().over(window_spec).alias(\"rn\"))\n",
        "\n",
        "      # Filtrar el usuario con mas tweets\n",
        "      .filter(col(\"rn\") == 1)\n",
        "\n",
        "      # Elimiar la columna utilizada\n",
        "      .drop(\"rn\")\n",
        "    )\n",
        "\n",
        "    # Se capura el resultado y se transforma en una lista de tuplas.\n",
        "    return [tuple(row) for row in day_user.collect()]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8TGHkTTg1EG",
        "outputId": "421c2a93-c618-47b8-f237-daf3d0bdc114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: /content/q1_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     5    309.2 MiB    309.2 MiB           1   @memory_profiler.profile\n",
            "     6                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "     7                                           \"\"\"\n",
            "     8                                           Obtiene los usuarios con más tweets por día para los 10 días con mayor actividad.\n",
            "     9                                           \n",
            "    10                                           Esta función lee un archivo JSON de tweets, identifica los 10 días con \n",
            "    11                                           mayor cantidad de tweets y determina el usuario con más tweets para cada \n",
            "    12                                           uno de esos días. La función está optimizada para la memoria usada en el  \n",
            "    13                                           procesamiento utilizando Spark para el análisis distribuido de datos.\n",
            "    14                                           \n",
            "    15                                           Args:\n",
            "    16                                             file_path (str): La ruta al archivo JSON que contiene los tweets.\n",
            "    17                                           \n",
            "    18                                           Returns:\n",
            "    19                                             List[Tuple[datetime.date, str]]: Una lista de tuplas donde cada tupla \n",
            "    20                                             contiene la fecha (datetime.date) y el nombre de usuario (str) \n",
            "    21                                             del usuario con más tweets para ese día.\n",
            "    22                                           \n",
            "    23                                           Raises:\n",
            "    24                                             FileNotFoundError: Si el archivo especificado en `file_path` no se encuentra.\n",
            "    25                                             Exception: Si ocurre cualquier otro error durante el procesamiento.\n",
            "    26                                           \"\"\"\n",
            "    27    309.2 MiB      0.0 MiB           1     from pyspark.sql import SparkSession\n",
            "    28    309.2 MiB      0.0 MiB           1     from pyspark.sql.functions import col, to_date, to_timestamp, row_number, desc, col\n",
            "    29    309.2 MiB      0.0 MiB           1     from pyspark.sql.window import Window\n",
            "    30    309.2 MiB      0.0 MiB           1     from pyspark.sql.utils import AnalysisException\n",
            "    31                                         \n",
            "    32    309.2 MiB      0.0 MiB           1     try:\n",
            "    33                                         \n",
            "    34    309.2 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(\"TweetAnalysis\").getOrCreate()\n",
            "    35    309.2 MiB      0.0 MiB           1       dfRaw = spark.read.json(file_path)\n",
            "    36                                         \n",
            "    37                                             # Se extrae la fecha y el usuario, convirtiendo la fecha al formato correcto.\n",
            "    38    309.2 MiB      0.0 MiB           2       df = dfRaw.select(\n",
            "    39    309.2 MiB      0.0 MiB           1           col(\"user.username\").alias(\"username\"),\n",
            "    40    309.2 MiB      0.0 MiB           1           to_date(to_timestamp(col(\"date\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\")).alias(\"date\"),\n",
            "    41                                               )\n",
            "    42                                         \n",
            "    43                                             # Se obtienen los top N días con más tweets.\n",
            "    44    309.2 MiB      0.0 MiB           1       top_dates = (\n",
            "    45    309.2 MiB      0.0 MiB           1           df.groupBy(\"date\")\n",
            "    46    309.2 MiB      0.0 MiB           1           .count()\n",
            "    47    309.2 MiB      0.0 MiB           1           .orderBy(desc(\"count\"))\n",
            "    48    309.2 MiB      0.0 MiB           1           .select(\"date\")\n",
            "    49    309.2 MiB      0.0 MiB           1           .limit(10)\n",
            "    50    309.2 MiB      0.0 MiB           1           .rdd.flatMap(lambda x: x)\n",
            "    51    309.2 MiB      0.0 MiB           1           .collect()\n",
            "    52                                               )\n",
            "    53                                         \n",
            "    54                                             # Se define la ventana para obtener el usuario con más tweets por día.\n",
            "    55    309.2 MiB      0.0 MiB           1       window_spec = Window.partitionBy(\"date\").orderBy(desc(\"count\"))\n",
            "    56                                         \n",
            "    57                                             # Se filtra por los top N días, se agrupa por fecha y usuario, se cuenta\n",
            "    58                                             # y se selecciona el usuario con más tweets (rn = 1).\n",
            "    59    309.2 MiB      0.0 MiB           1       day_user = (\n",
            "    60    309.2 MiB      0.0 MiB           1         df.filter(col(\"date\").isin(top_dates))\n",
            "    61    309.2 MiB      0.0 MiB           1         .groupBy(\"date\", \"username\")\n",
            "    62    309.2 MiB      0.0 MiB           1         .count()\n",
            "    63    309.2 MiB      0.0 MiB           1         .select(col(\"date\"), col(\"username\"), row_number().over(window_spec).alias(\"rn\"))\n",
            "    64    309.2 MiB      0.0 MiB           1         .filter(col(\"rn\") == 1)\n",
            "    65    309.2 MiB      0.0 MiB           1         .drop(\"rn\")\n",
            "    66                                             )\n",
            "    67                                         \n",
            "    68                                             # Se devuelve el resultado como una lista de tuplas.\n",
            "    69    309.2 MiB      0.0 MiB          13       return [tuple(row) for row in day_user.collect()]\n",
            "    70                                         \n",
            "    71                                           except FileNotFoundError:\n",
            "    72                                             print(f\"Error: El archivo '{file_path}' no se encuentra.\")\n",
            "    73                                             raise\n",
            "    74                                           except Exception as e:\n",
            "    75                                             print(f\"Error durante el procesamiento: {e}\")\n",
            "    76                                             raise\n",
            "    77                                           finally:\n",
            "    78    309.2 MiB      0.0 MiB           1       spark.stop()\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
              " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
              " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
              " (datetime.date(2021, 2, 15), 'jot__b'),\n",
              " (datetime.date(2021, 2, 16), 'jot__b'),\n",
              " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
              " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
              " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
              " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
              " (datetime.date(2021, 2, 23), 'Surrypuria')]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q1_memory.q1_memory(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z2vWtVo9UhM",
        "outputId": "193595bc-566e-4f7b-a3f1-2b00ee4386aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Sampling process 100 times a second. Press Control-C to exit.\n",
            "\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/02/13 06:50:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.00s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.38s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.87s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.85s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.11s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.65s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.74s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.48s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.64s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.61s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.38s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.83s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.91s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.32s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.05s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.06s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.00s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.69s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.03s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.01s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.01s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.18s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Stopped sampling because process exited\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Wrote flamegraph data to 'q1_memory.svg'. Samples: 2869 Errors: 0\n"
          ]
        }
      ],
      "source": [
        "!py-spy record -o q1_memory.svg -- python -c \"import os;  q1_memory; q1_memory.q1_memory(os.environ.get('FILE_PATH'))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUFzqizHCPoX"
      },
      "source": [
        "## Resultados q1\n",
        "\n",
        "* la creacion de la session de spak agrega un tiempo considerable a cada funcion\n",
        "* la lectura del json en cada caso es la operacion mas costosa\n",
        "* La version **optimizada para tiempo es mas lenta** debido a la escritura del cache y la funcion de reduceByKey\n",
        "* Las dos versiones tienen un consumo de memoria parecido\n",
        "* Se deberia analizar los recursos utilizados dentro del framework de pyspark para tener mas detalle sobre las transformaciones mas costosas\n",
        "* Se deberia ver la duracion de las funciones con un dataset muchisimo mas grande para ver la difencia que hace la aplicacion del cache en el df\n",
        "* los resultados de *py-spy* estan en la carpeta *resultados_py-spy*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8V5yNRpLpK0"
      },
      "source": [
        "## q2\n",
        "\n",
        "### Consideraciones\n",
        "* En un entorno de BigData pueden llegar grandes cantidades de datos en Streaming.\n",
        "* Se utilizo Beam y PySpark para comparar los recursos utilizados, sin embargo al momento de agregar el decorador @memory_profiler.profile a la solucion con Beam el monitoreo de recursos impedia la correcta ejecucion del pipeline por lo que no se agrego\n",
        "* Los dos soportan grandes cantidades de datos y son escalables\n",
        "* Beam puede ser facilmente implementado en Data Flow\n",
        "* PySpark puede ser facilmente implementado en DataProc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAorOY1ifvVJ"
      },
      "outputs": [],
      "source": [
        "import q2_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkhYce_AE4cp"
      },
      "source": [
        "#### q2_time\n",
        "\n",
        "* Se crea un pipeline en beam que escriba el resultado en un archivo de texto que luego sera leido e interpretado como tupla\n",
        "*   La logica utilizada fue:\n",
        " 1. capturar linea por linea del contenido del tweet\n",
        " 2. comprar cada palabra para saber si es emoji\n",
        " 3. Agrupar por elemento para contarlos en un contador\n",
        " 4. Tomar los 10 primeros\n",
        " 5. Escribir el resultado en archivo plano (no se puede transformar directo a tupla)\n",
        " 6. Leer el archivo y dar la respuesta\n",
        "\n",
        " #### Explicación código\n",
        "\n",
        "\n",
        "\n",
        "``` py\n",
        "\n",
        "      # Crear el pipeline para no tener que agregar variable para esperar el status de la ejecucion\n",
        "      with beam.Pipeline() as pipeline:\n",
        "          \n",
        "          # 1. Leer los tweets desde el archivo JSON\n",
        "          tweets = (\n",
        "              pipeline\n",
        "              # Leer archivo txt\n",
        "              | \"LeerTweets\" >> beam.io.ReadFromText(file_path)\n",
        "              # Leer cada linea como Json\n",
        "              | \"ParsearJSON\" >> beam.Map(lambda line: json.loads(line))\n",
        "          )\n",
        "\n",
        "          # 2. Extraer los emojis de cada tweet\n",
        "          emojis = (\n",
        "              tweets\n",
        "\n",
        "              # Recorrer el contenido del tweet palabra por palabra, si una de esas palabras es un emoji entonces devolverla\n",
        "              | \"ExtraerEmojis\" >> beam.FlatMap(lambda tweet: [char for char in tweet.get('content', '') if emoji.is_emoji(char)])\n",
        "          )\n",
        "\n",
        "          # 3. Contar la frecuencia de cada emoji\n",
        "          emoji_counts = (\n",
        "              emojis\n",
        "\n",
        "              # Contar cuantas veces aparece cada emoji, un conteo por cada elemento\n",
        "              | \"ContarEmojis\" >> beam.combiners.Count.PerElement()\n",
        "              | \"FormatearSalida\" >> beam.Map(lambda element: element)\n",
        "          )\n",
        "          \n",
        "          # 4. Obtener los 10 emojis más frecuentes\n",
        "          top_10_emojis = (\n",
        "              emoji_counts\n",
        "\n",
        "              # Devuelve los 10 primeros elementos con mas conteos, el conteo esta en la segunda \"columna\" por eso se requiere agregar lambda para que sepa la funcion para buscar\n",
        "              | \"OrdenarEmojis\" >> beam.transforms.combiners.Top.Of(10, key=lambda element: element[1])\n",
        "          )\n",
        "\n",
        "          # 5. Escribir los resultados a un archivo de texto\n",
        "          top_10_emojis | 'EscribirResultados' >> beam.io.WriteToText(f'{folder_path}output.txt', num_shards=1, shard_name_template=\"\")\n",
        "\n",
        "      # 6. Leer el archivo de texto y evaluarlo literalmente    \n",
        "      with open(f'{folder_path}output.txt', 'r') as f:\n",
        "            text_field_content = f.read()\n",
        "      list_of_tuples = ast.literal_eval(text_field_content)\n",
        "\n",
        "      # 7. Eliminar el archivo\n",
        "      shutil.rmtree(folder_path)\n",
        "      return list_of_tuples\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHL1INEVFnKa"
      },
      "outputs": [],
      "source": [
        "q2_time.q2_time(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOrQlqmQiI7V",
        "outputId": "ce65a529-fa02-46e3-99d9-3d3687fb4106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Sampling process 100 times a second. Press Control-C to exit.\n",
            "\n",
            "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.transforms.combiners._TopPerBundle'>)\n",
            "\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Stopped sampling because process exited\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Wrote flamegraph data to 'q2_time.svg'. Samples: 3995 Errors: 1\n"
          ]
        }
      ],
      "source": [
        "!py-spy record -o q2_time.svg -- python -c \"import os;  q2_time; q2_time.q2_time(os.environ.get('FILE_PATH'))\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTKjgALzNTwc"
      },
      "outputs": [],
      "source": [
        "import q2_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tax8qC-90Hm6"
      },
      "source": [
        "#### q2_memory\n",
        "\n",
        "* Se implementan transforamciones usando pyspark con una logica muy parecida a la implementada en beam.\n",
        "* Se creo una *udf* para hacer la comparacion palabra por palabra y saber si es o no un emoji\n",
        "*   La logica utilizada fue:\n",
        " 1. Crear la sesion de spark\n",
        " 2. Leer el archivo Json\n",
        " 3. Aplicar a la columna content la UDF que compara palabra por palabra.\n",
        " 4. Aplicar un explode a lo anterior para obtener una unica columna con todos los emojis\n",
        " 5. Agrupar por emoji y contarlos\n",
        " 6. Ordernalos en forma descendente\n",
        " 7. Tomar los 10 primeros\n",
        " 8. Colectarlos\n",
        " 9. converitr fila por fila en una tupla y luego en lista\n",
        "\n",
        " #### Explicación código\n",
        "\n",
        "\n",
        "``` py\n",
        "      # Crear sesion de spark\n",
        "      spark = SparkSession.builder.appName(\"TweetAnalysis\").getOrCreate()\n",
        "\n",
        "      #Leer archivo Json\n",
        "      dfRaw = spark.read.json(file_path)\n",
        "\n",
        "      # Define una UDF para extraer los emojis del texto\n",
        "\n",
        "      # Se crea la funcion que validara los emojis\n",
        "      def extract_emojis(text):\n",
        "        \n",
        "        # Si el texto contiene algo entonces:\n",
        "          if text is not None:\n",
        "\n",
        "              # revisa palabra por palabra del texto y valida si es emoji, si es emoji devuelve la palabra\n",
        "              return [char for char in text if emoji.is_emoji(char)]\n",
        "          return []\n",
        "\n",
        "      # Se crea la UDF con la funcion anterior\n",
        "      extract_emojis_udf = udf(extract_emojis, ArrayType(StringType()))\n",
        "\n",
        "      # Aplica UDF para extraer emojis y explode para transformarlos en columna\n",
        "      emoji_counts = (\n",
        "          dfRaw\n",
        "\n",
        "          # Se aplica la UDF a la columna content, luego explode para que quede una sola columna con los resultados\n",
        "          .select(explode(extract_emojis_udf(col(\"content\"))).alias(\"emoji\"))\n",
        "          \n",
        "          # Se cuenta agrupando por emoji\n",
        "          .groupBy(\"emoji\")\n",
        "          .count()\n",
        "          \n",
        "          # Se ordena de forma descendente\n",
        "          .orderBy(col(\"count\").desc())\n",
        "      )\n",
        "\n",
        "      # Obtiene los top 10 y lo convierte a una lista de tuplas\n",
        "      # Se toman las 10 primeras, se colecta y se transforma fila por fila en una tupla\n",
        "      top_10_emojis = [tuple(row) for row in emoji_counts.limit(10).collect()]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2RunOxfySt4",
        "outputId": "016dbe74-09d3-4bd6-ae18-eeb62e99f83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: /content/q2_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     7    308.5 MiB    308.5 MiB           1   @memory_profiler.profile\n",
            "     8                                         def q2_memory(file_path):\n",
            "     9                                             \"\"\"\n",
            "    10                                             Procesa un archivo JSON de tweets, extrae emojis y retorna los 10 mas usados.\n",
            "    11                                         \n",
            "    12                                             Args:\n",
            "    13                                                 file_path (str): La ruta al archivo JSON.\n",
            "    14                                         \n",
            "    15                                             Returns:\n",
            "    16                                                 List[Tuple[str, int]]: Una lista de tuplas (emoji, conteo) representando\n",
            "    17                                                       los 10 emojis mas usados.\n",
            "    18                                             \"\"\"\n",
            "    19    308.5 MiB      0.0 MiB           1       try:\n",
            "    20    308.5 MiB      0.0 MiB           1           spark = SparkSession.builder.appName(\"TweetAnalysis\").getOrCreate()\n",
            "    21    308.5 MiB      0.0 MiB           1           dfRaw = spark.read.json(file_path)\n",
            "    22                                         \n",
            "    23                                                 # Define una UDF para extraer los emojis del texto\n",
            "    24    308.5 MiB      0.0 MiB           1           def extract_emojis(text):\n",
            "    25                                                     if text is not None:\n",
            "    26                                                         return [char for char in text if emoji.is_emoji(char)]\n",
            "    27                                                     return []\n",
            "    28                                         \n",
            "    29    308.5 MiB      0.0 MiB           1           extract_emojis_udf = udf(extract_emojis, ArrayType(StringType()))\n",
            "    30                                         \n",
            "    31                                                 # Aplica UDF para extraer emojis y explode para transformarlos en columna\n",
            "    32    308.5 MiB      0.0 MiB           1           emoji_counts = (\n",
            "    33    308.5 MiB      0.0 MiB           1               dfRaw\n",
            "    34    308.5 MiB      0.0 MiB           1               .select(explode(extract_emojis_udf(col(\"content\"))).alias(\"emoji\"))\n",
            "    35    308.5 MiB      0.0 MiB           1               .groupBy(\"emoji\")\n",
            "    36    308.5 MiB      0.0 MiB           1               .count()\n",
            "    37    308.5 MiB      0.0 MiB           1               .orderBy(col(\"count\").desc())\n",
            "    38                                                 )\n",
            "    39                                         \n",
            "    40                                                 # Obtiene los top 10 y lo convierte a una lista de tuplas\n",
            "    41    308.5 MiB      0.0 MiB          13           top_10_emojis = [tuple(row) for row in emoji_counts.limit(10).collect()]\n",
            "    42                                         \n",
            "    43    308.5 MiB      0.0 MiB           1           return top_10_emojis\n",
            "    44                                         \n",
            "    45                                             except FileNotFoundError:\n",
            "    46                                                 print(f\"Error: El archivo '{file_path}' no se encuentra.\")\n",
            "    47                                                 raise\n",
            "    48                                             except Exception as e:\n",
            "    49                                                 print(f\"Error durante el procesamiento: {e}\")\n",
            "    50                                                 raise\n",
            "    51                                             finally:\n",
            "    52    308.6 MiB      0.0 MiB           1         spark.stop()\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('🙏', 7286),\n",
              " ('😂', 3072),\n",
              " ('🚜', 2972),\n",
              " ('✊', 2411),\n",
              " ('🌾', 2363),\n",
              " ('🏻', 2080),\n",
              " ('❤', 1779),\n",
              " ('🤣', 1668),\n",
              " ('🏽', 1218),\n",
              " ('👇', 1108)]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q2_memory.q2_memory(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymPj4XWXimTT",
        "outputId": "ef5d27e3-b22f-4966-d861-d1224133f2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Sampling process 100 times a second. Press Control-C to exit.\n",
            "\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/02/13 05:33:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.09s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.66s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.75s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.40s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.01s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.10s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.20s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.07s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.59s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.64s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.01s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.37s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.84s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.55s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.24s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.22s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.06s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.32s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.72s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.20s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 5.03s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 5.85s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 6.48s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 7.39s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 7.68s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 7.65s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 7.97s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 8.70s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 8.55s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 7.51s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 6.76s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 5.21s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.85s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.00s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.87s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.09s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.17s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.31s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.36s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.02s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.27s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 1.87s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 2.56s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.23s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.62s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.74s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.21s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.86s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.86s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.92s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 5.47s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.25s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 4.00s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.67s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.41s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m 3.21s behind in sampling, results may be inaccurate. Try reducing the sampling rate\n",
            "\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Stopped sampling because process exited\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Wrote flamegraph data to 'q2_memory.svg'. Samples: 1584 Errors: 0\n",
            "Error: No child process (os error 10)\n"
          ]
        }
      ],
      "source": [
        "!py-spy record -o q2_memory.svg -- python -c \"import os;  q2_memory; q2_memory.q2_memory(os.environ.get('FILE_PATH'))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz224H1Sykce"
      },
      "source": [
        "## Resultados q2\n",
        "* En este caso Beam fue mucho mas rapido que Spark.\n",
        "* Spark estuvo mas de la mitad del tiempo trabajando con hilos (_bootstrap_inner (threading.py:1045))\n",
        "* Se deberia incrementar la carga para ver como se comporta Beam ante una gran cantidad de datos en Batch\n",
        "* Se necesitan herramientas puntuales para valorar el consumo de memoria en ambos casos\n",
        "* los resultados de *py-spy* estan en la carpeta *resultados_py-spy*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLTtCoeqMrFe"
      },
      "source": [
        "## q3\n",
        "\n",
        "### Consideraciones\n",
        "* Se utilizo Beam y Librerias propias de Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS5a1Kty5fmc"
      },
      "outputs": [],
      "source": [
        "import q3_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi5jpWIhNzPS"
      },
      "source": [
        "#### q3_time\n",
        "\n",
        "* Se implementa librerias propias de Python\n",
        "* La lectura se hace linea por linea con el fin de mejorar los tiempos de respuesta y el consumo de memoria\n",
        "* Se prodria trabajar con hilos dentro de python para mejorar aun mas el rendimiento deacuerdo a los recursos disponibles\n",
        "*   La logica utilizada fue:\n",
        " 1. Abrir el archivo\n",
        " 2. Recorrer el archivo linea\n",
        " 3. Convertir cada linea en Json\n",
        " 4. Verificar si el tweet tiene usuarios mencionados\n",
        " 5. Recorrer los usuarios mencionados\n",
        " 6. Capturar el nombre de usuario\n",
        " 7. Agregarlo al contador\n",
        " 8. Mostrar los 10 mas comunes\n",
        "\n",
        " #### Explicación código:\n",
        "\n",
        "\n",
        "``` py\n",
        "    # Abrir el archivo\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      \n",
        "      # Recorer linea por linea\n",
        "      for line in file:\n",
        "\n",
        "          # Cargar la linea como Json\n",
        "          tweet = json.loads(line)\n",
        "\n",
        "          # Verifica si hay mentionedUsers en el tweet\n",
        "          # Verifica si es una lista, es decir que no sea null\n",
        "          if 'mentionedUsers' in tweet and isinstance(tweet['mentionedUsers'], list):\n",
        "            \n",
        "            # Recorre los usuarios mencionados en cada Tweet\n",
        "            for user in tweet['mentionedUsers']:\n",
        "\n",
        "              # Verifica si el usuario tiene un nombre de usuario\n",
        "              if 'username' in user:\n",
        "                  \n",
        "                  #Lo agrega al contador\n",
        "                  user_mentions[user['username']] += 1\n",
        "    user_mentions.most_common(10)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bzRLvAk1IFd",
        "outputId": "9bcad889-de88-4f9e-a181-308c4f427248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: /content/q3_time.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     6    307.5 MiB    307.5 MiB           1   @memory_profiler.profile\n",
            "     7                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
            "     8                                             \"\"\"\n",
            "     9                                             Encuentra los 10 usuarios más mencionados en un archivo JSON de tweets.\n",
            "    10                                         \n",
            "    11                                             Args:\n",
            "    12                                                 file_path: La ruta al archivo JSON.\n",
            "    13                                         \n",
            "    14                                             Returns:\n",
            "    15                                                 Una lista de tuplas, donde cada tupla contiene el nombre de usuario y el número de menciones.\n",
            "    16                                             \"\"\"\n",
            "    17    307.5 MiB      0.0 MiB           1       user_mentions = Counter()\n",
            "    18    307.5 MiB      0.0 MiB           1       try:\n",
            "    19                                         \n",
            "    20                                               # Abrir el archivo\n",
            "    21    307.5 MiB      0.0 MiB           2         with open(file_path, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
            "    22                                                 \n",
            "    23                                                 # Recorer linea por linea\n",
            "    24    307.5 MiB      0.0 MiB      117408           for line in file:\n",
            "    25                                         \n",
            "    26    307.5 MiB      0.0 MiB      117407               try:\n",
            "    27                                                       #Cargar la linea como Json\n",
            "    28    307.5 MiB      0.0 MiB      117407                 tweet = json.loads(line)\n",
            "    29                                         \n",
            "    30                                                       #Verifica si hay mentionedUsers en el tweet\n",
            "    31                                                       #Verifica si es una lista, es decir que no sea null\n",
            "    32    307.5 MiB      0.0 MiB      117407                 if 'mentionedUsers' in tweet and isinstance(tweet['mentionedUsers'], list):\n",
            "    33                                                         \n",
            "    34                                                         #Recorre los usuarios mencionados en cada Tweet\n",
            "    35    307.5 MiB      0.0 MiB      141437                   for user in tweet['mentionedUsers']:\n",
            "    36                                         \n",
            "    37                                                           #Verifica si el usuario tiene un nombre de usuario\n",
            "    38    307.5 MiB      0.0 MiB      103403                     if 'username' in user:\n",
            "    39                                                               \n",
            "    40                                                               #Lo agrega al contador\n",
            "    41    307.5 MiB      0.0 MiB      103403                         user_mentions[user['username']] += 1                    \n",
            "    42                                                     except json.JSONDecodeError as e:\n",
            "    43                                                         print(f\"Linea de Json invalida, pasa a la siguiente: {e}\")\n",
            "    44                                             except FileNotFoundError:\n",
            "    45                                                 print(f\"Archivo no encontrado {file_path}\")\n",
            "    46                                                 raise\n",
            "    47                                             except Exception as e:\n",
            "    48                                                 print(f\"Error inesperado: {e}\")\n",
            "    49                                                 raise\n",
            "    50                                         \n",
            "    51                                             #Retorna los 1o usuarios mas mencionados\n",
            "    52    307.5 MiB      0.0 MiB           1       return user_mentions.most_common(10)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('narendramodi', 2265),\n",
              " ('Kisanektamorcha', 1840),\n",
              " ('RakeshTikaitBKU', 1644),\n",
              " ('PMOIndia', 1427),\n",
              " ('RahulGandhi', 1146),\n",
              " ('GretaThunberg', 1048),\n",
              " ('RaviSinghKA', 1019),\n",
              " ('rihanna', 986),\n",
              " ('UNHumanRights', 962),\n",
              " ('meenaharris', 926)]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q3_time.q3_time(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvbhBK3m69xn",
        "outputId": "148f83e1-424c-4cb1-fa6c-46402a5b6f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Sampling process 100 times a second. Press Control-C to exit.\n",
            "\n",
            "\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Stopped sampling because process exited\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Wrote flamegraph data to 'q3_time.svg'. Samples: 809 Errors: 0\n"
          ]
        }
      ],
      "source": [
        "!py-spy record -o q3_time.svg -- python -c \"import os;  q3_time; q3_time.q3_time(os.environ.get('FILE_PATH'))\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgbI9GVKKgcF"
      },
      "outputs": [],
      "source": [
        "import q3_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUq-yrK9bHO"
      },
      "source": [
        "#### q3_memory\n",
        "\n",
        "* Se implementa un pipeline de Beam con una logica muy parecida a la que se utilizo para el caso *q3_time*\n",
        "* Se escribe un archivo de salida para obtener el resultado del pipeline\n",
        "*   La logica utilizada fue:\n",
        " 1. Leer el archivo\n",
        " 2. Cargar cada linea como Json\n",
        " 3. Filtrar los solo los tweets que tienen usuarios mencionados\n",
        " 4. Extraer los nombres de usuarios de la lista de usuarios mencionados\n",
        " 5. Hacer un conteo por elemento\n",
        " 6. Tomar los 10 primeros elementos con mas conteos (x[1])\n",
        " 7. Escribir los resultados en un archivo\n",
        " 8. Leer el archivo e interpretarlo literalmente\n",
        " 9. Eliminar el archivo creado\n",
        "\n",
        " #### Explicación código:\n",
        "\n",
        "\n",
        "\n",
        "``` py\n",
        "    # Crear el pipeline\n",
        "    with beam.Pipeline() as pipeline:\n",
        "\n",
        "      # Lee y analiza los tweets, extrae las menciones de usuarios, toma las 10 y escribe el resultado\n",
        "      (\n",
        "        pipeline\n",
        "        | 'ReadTweets' >> beam.io.ReadFromText(file_pattern=file_path)\n",
        "        | 'ParseTweets' >> beam.Map(lambda line: json.loads(line))\n",
        "        | 'FilterTweets' >> beam.Filter(lambda tweet: tweet.get('mentionedUsers') is not None)\n",
        "        | 'ExtractMentions' >> beam.FlatMap(lambda tweet: [user['username'] for user in tweet.get('mentionedUsers')])\n",
        "        | 'CountMentions' >> beam.combiners.Count.PerElement()\n",
        "        | 'SortMentions' >> beam.combiners.Top.Of(10, key=lambda x: x[1])\n",
        "        | 'WriteResults' >> beam.io.WriteToText(f'{folder_path}output.txt', num_shards=1, shard_name_template=\"\")\n",
        "      )\n",
        "\n",
        "    # Leer el archivo de texto y evaluarlo literalmente\n",
        "    with open(f'{folder_path}output.txt', 'r') as f:\n",
        "        text_field_content = f.read()\n",
        "    list_of_tuples = ast.literal_eval(text_field_content)\n",
        "\n",
        "    # Eliminar el archivo\n",
        "    shutil.rmtree(folder_path)\n",
        "\n",
        "    return list_of_tuples\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lobXplx9e2o",
        "outputId": "ce2a75bc-1bd7-40f9-aa6e-737b1cf98f01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.transforms.combiners._TopPerBundle'>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: /content/q3_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     8    307.5 MiB    307.5 MiB           1   @memory_profiler.profile\n",
            "     9                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "    10                                             \"\"\"\n",
            "    11                                             Encuentra los 10 usuarios más mencionados en un archivo JSON de tweets usando Apache Beam.\n",
            "    12                                         \n",
            "    13                                             Args:\n",
            "    14                                                 file_path: La ruta al archivo JSON.\n",
            "    15                                         \n",
            "    16                                             Returns:\n",
            "    17                                                 Una lista de tuplas, donde cada tupla contiene el nombre de usuario y el número de menciones.\n",
            "    18                                             \"\"\"\n",
            "    19    307.5 MiB      0.0 MiB           1       try:\n",
            "    20                                         \n",
            "    21    307.5 MiB      0.0 MiB           1         folder_path = \"q3/\"\n",
            "    22                                         \n",
            "    23    307.5 MiB     -7.0 MiB           2         with beam.Pipeline() as pipeline:\n",
            "    24                                         \n",
            "    25                                                 # Lee y analiza los tweets, extrae las menciones de usuarios, toma las 10 y escribe el resultado\n",
            "    26    307.1 MiB     -0.5 MiB           1           top_mentions = (\n",
            "    27    307.5 MiB     -0.5 MiB           8               pipeline\n",
            "    28    307.5 MiB      0.0 MiB           1               | 'ReadTweets' >> beam.io.ReadFromText(file_pattern=file_path)\n",
            "    29    310.3 MiB      2.5 MiB      234815               | 'ParseTweets' >> beam.Map(lambda line: json.loads(line))\n",
            "    30    310.3 MiB     -0.2 MiB      234815               | 'FilterTweets' >> beam.Filter(lambda tweet: tweet.get('mentionedUsers') is not None)\n",
            "    31    310.3 MiB     -0.5 MiB      255540               | 'ExtractMentions' >> beam.FlatMap(lambda tweet: [user['username'] for user in tweet.get('mentionedUsers')])\n",
            "    32    307.1 MiB     -0.5 MiB           1               | 'CountMentions' >> beam.combiners.Count.PerElement()\n",
            "    33    313.1 MiB   -149.7 MiB       30499               | 'SortMentions' >> beam.combiners.Top.Of(10, key=lambda x: x[1])\n",
            "    34    307.1 MiB     -0.5 MiB           1               | 'WriteResults' >> beam.io.WriteToText(f'{folder_path}output.txt', num_shards=1, shard_name_template=\"\")\n",
            "    35                                                 )\n",
            "    36                                         \n",
            "    37                                               # Leer el archivo de texto y evaluarlo literalmente\n",
            "    38    306.1 MiB     -1.4 MiB           2         with open(f'{folder_path}output.txt', 'r') as f:\n",
            "    39    306.1 MiB      0.0 MiB           1             text_field_content = f.read()\n",
            "    40    306.1 MiB      0.0 MiB           1         list_of_tuples = ast.literal_eval(text_field_content)\n",
            "    41                                         \n",
            "    42                                               # Eliminar el archivo\n",
            "    43    306.1 MiB      0.0 MiB           1         shutil.rmtree(folder_path)\n",
            "    44                                         \n",
            "    45    306.1 MiB      0.0 MiB           1         return list_of_tuples\n",
            "    46                                                   \n",
            "    47                                             except (IOError, json.JSONDecodeError) as e:\n",
            "    48                                                 print(f\"Error procesando el archivo: {e}\")\n",
            "    49                                                 return []\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('narendramodi', 2265),\n",
              " ('Kisanektamorcha', 1840),\n",
              " ('RakeshTikaitBKU', 1644),\n",
              " ('PMOIndia', 1427),\n",
              " ('RahulGandhi', 1146),\n",
              " ('GretaThunberg', 1048),\n",
              " ('RaviSinghKA', 1019),\n",
              " ('rihanna', 986),\n",
              " ('UNHumanRights', 962),\n",
              " ('meenaharris', 926)]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q3_memory.q3_memory(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLv-kPCt-Q_R",
        "outputId": "467bf23e-d3fa-4990-b3d5-0024c946ce64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Sampling process 100 times a second. Press Control-C to exit.\n",
            "\n",
            "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.transforms.combiners._TopPerBundle'>)\n",
            "\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Stopped sampling because process exited\n",
            "\u001b[32m\u001b[1mpy-spy\u001b[0m\u001b[2m>\u001b[0m Wrote flamegraph data to 'q3_memory.svg'. Samples: 3347 Errors: 3\n"
          ]
        }
      ],
      "source": [
        "!py-spy record -o q3_memory.svg -- python -c \"import os;  q3_memory; q3_memory.q3_memory(os.environ.get('FILE_PATH'))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x3hs16u_bem"
      },
      "source": [
        "## Resultados q3\n",
        "* A *q3_time* le toma 805 samples para completar, siendo en su mayoria el tiempo de apertura del archivo y la lectura por linea, mientras que a *q3_memory* le toma 3200 con una parte dedicada las librerias requeridas para el funcionamiento, siendo 2500 para el procesamiento como tal\n",
        "* buena parte del tiempo de *q3_memory* en la lectura del archivo Json y otra en la escritura del archivo de texto\n",
        "* los resultados de *py-spy* estan en la carpeta *resultados_py-spy*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRs-8yCZ-fNO"
      },
      "source": [
        "# Mejoras\n",
        "\n",
        "* Agregar pruebas unitaras.\n",
        "* Agregar sistemas de monitoreo, sistema de log.\n",
        "* Documentar de forma estructurada las 3 funciones (entradas, salidas objetivos).\n",
        "* Automatizar proceso de despligue CI/CD.\n",
        "* Validar seguridad."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
